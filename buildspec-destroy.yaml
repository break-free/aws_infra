# resource
# https://www.padok.fr/en/blog/codepipeline-eks-helm

version: 0.2

env:
  variables:
    PROJECT_NAME: "company-a"
    #TODO: add list of users
    USERS: "arn:aws:iam::146452989591:user/Han.Tu"
    # run AWS.exe by 
    # & $env:AWSCLI
    AWSCLI: 'C:\Program Files\Amazon\AWSCLI\bin\aws.exe'
  #parameter-store:
     # key: "value"
     # key: "value"
  #secrets-manager:
     # key: secret-id:json-key:version-stage:version-id
     # key: secret-id:json-key:version-stage:version-id
  #exported-variables:
     # - variable
     # - variable
  #git-credential-helper: yes

phases:
  install: # Install Chocolatey, AWS CLI, Terraform, Kubectl
    commands:
      # Chocolatey 
      - "Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))"
      # AWS CLI, Terraform, Kubectl
      - choco install -yvf terraform kubernetes-cli
      - choco upgrade -yvf awscli --version 1.18.6 # AWS build server contains 1.14.x, which predates EKS, and this choco installs breaks with AWSCLI 2+, so we're stuck with one of the last 1.x
  pre_build:
    commands:
  build:
    commands:
       # point kubectl to eks
      - "& $AWSCLI eks --region $env:AWS_REGION update-kubeconfig --name $PROJECT_NAME"
      - "$(get-content ~\\.kube\\config)  -replace '(command: )(.*$)',\"`${1}$AWSCLI\" | Set-Content -path  ~\\.kube\\config"  #replace w/ full AWSCLI path 
      
      # app 
      - set-location app
      # run TF for Destroy app
      - .\app_setup.ps1 $PROJECT_NAME $env:AWS_REGION
      - terraform init -input=false
      - terraform destroy -auto-approve

      # EKS
      - set-location ..\eks\terraform
      # run TF for EKS
      - .\terraform_setup.ps1 $PROJECT_NAME $env:AWS_REGION
      - terraform init -input=false
      - terraform destroy -auto-approve

      # Remote backend
      - set-location ..\..\remote-backend
      - .\backend_setup.ps1 $PROJECT_NAME $env:AWS_REGION
      - terraform init -input=false
      # import existing remote backend 
      - if( & $AWSCLI s3api list-buckets --query "Buckets[].Name" | select-string -Pattern "`"$PROJECT_NAME-state`"") {terraform import aws_s3_bucket.tf-state-storage $PROJECT_NAME-state}
      - if( & $AWSCLI s3api list-buckets --query "Buckets[].Name" | select-string -Pattern "`"$PROJECT_NAME-app-state`"") {terraform import aws_s3_bucket.tf-state-storage-app $PROJECT_NAME-app-state}
      - if( & $AWSCLI dynamodb list-tables --query "TableNames[*]" | select-string -Pattern "`"$PROJECT_NAME-state-lock`"") {terraform import aws_dynamodb_table.dynamodb-terraform-state-lock $PROJECT_NAME-state-lock}
      - if( & $AWSCLI dynamodb list-tables --query "TableNames[*]" | select-string -Pattern "`"$PROJECT_NAME-state-lock-app`"") {terraform import aws_dynamodb_table.dynamodb-terraform-state-lock-app $PROJECT_NAME-state-lock-app}
      #run TF for backend
      #you'll need to manually delete the s3 buckets
      - terraform destroy --target aws_dynamodb_table.dynamodb-terraform-state-lock --target aws_dynamodb_table.dynamodb-terraform-state-lock-app -auto-approve
      - write "please manually destroy S3 buckets:`n$PROJECT_NAME-state`n$PROJECT_NAME-app-state"